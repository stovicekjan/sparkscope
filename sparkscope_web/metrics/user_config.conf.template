[thresholds_stage_skew]
# Minimum runtime (in milliseconds) of the stages which should be included
min_runtime=30000

# Values of maximum/median task runtime ratio, used as limits for LOW or HIGH severity
ratio_low=2.0
ratio_high=10.0

[thresholds_stage_disk_spill]
# Values of memory_spill/total_used_memory ratio, used as limits for LOW or HIGH severity
ratio_low=0.0
ratio_high=0.5

[thresholds_driver_gc]
# Values of GC_time/total_time for driver, used as limits for LOW or HIGH severity if the GC time is too low
low_ratio_low_severity=0.01
low_ratio_high_severity=0.003
# Values of GC_time/total_time for driver, used as limits for LOW or HIGH severity if the GC time is too high
high_ratio_low_severity=0.1
high_ratio_high_severity=0.2

[thresholds_executor_gc]
# Values of sum(GC_time)/sum(total_time) for executors, used as limits for LOW or HIGH severity if the GC time is too low
low_ratio_low_severity=0.01
low_ratio_high_severity=0.003
# Values of sum(GC_time)/sum(total_time) for executors, used as limits for LOW or HIGH severity if the GC time is too high
high_ratio_low_severity=0.1
high_ratio_high_severity=0.2

[configs_serializer]
# Preferred serializer to be used in Spark Applications
preferred_serializer=org.apache.spark.serializer.KryoSerializer

[configs_dynamic_allocation]
# is dynamic allocation the desired setup?
is_dynamic_allocation_preferred=True

[configs_dynamic_allocation_min_max_executors]
# minimum number of executors requested when dynamic allocation is enabled. The less, the better (so that no executors
# are wasted)
min_executors_low_severity=1
min_executors_high_severity=1

# maximum number of executors requested when dynamic allocation is enabled. The less, the better (so that the
# application does not consume all the resources that might be shared
max_executors_low_severity=1000
max_executors_high_severity=1000

[configs_yarn_queue]
# this setting should be True if the default YARN queue is allowed to be used. If you want to push users towards using
# separate queues (might be useful on shared clusters), set it to False
is_default_queue_allowed=False

[configs_memory_settings]
# maximum values of memory settings. If a Spark application uses more than that, an issue will be flagged.
max_executor_memory_low_severity=10g
max_executor_memory_high_severity=16g
max_executor_memory_overhead_low_severity=4g
max_executor_memory_overhead_high_severity=6g
max_driver_memory_low_severity=8g
max_driver_memory_high_severity=12g
max_driver_memory_overhead_low_severity=1g
max_driver_memory_overhead_high_severity=2g

[configs_cores]
# Values used as thresholds for lower limit of executor cores number, with low or high severity
executor_low_limit_low_severity=3
executor_low_limit_high_severity=2
# Values used as thresholds for higher limit of executor cores number, with low or high severity
executor_high_limit_low_severity=5
executor_high_limit_high_severity=6
# Values used as thresholds for lower limit of driver cores number, with low or high severity
driver_low_limit_low_severity=3
driver_low_limit_high_severity=2
# Values used as thresholds for higher limit of driver cores number, with low or high severity
driver_high_limit_low_severity=5
driver_high_limit_high_severity=6

[readable_list_length]
# Number of the most severe contributors to the issues to be rendered on the application page. -1 represents all.
stage_skew_readable_list_length=5
stage_disk_spill_readable_list_length=5
executor_gc_readable_list_length=5

